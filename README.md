# M1 CSV Validation Script - Proof of Concept

## Overview

This Python script provides a proof-of-concept (PoC) for validating records from an M1 CSV export (typically generated by a tool like Pozi Connect) against a council's rates database. The primary goal is to assist in the review process by automatically cross-referencing M1 proposed changes with information stored in the rates system, particularly data found in property 'Memo' fields.

The script, in its current PoC state:
1.  Downloads a sample M1 CSV file from a specified URL.
2.  Parses this CSV.
3.  Uses a **simulated** rates database (a Python list of dictionaries called `sample_rates_data`) for property lookups.
4.  Applies a set of validation rules to each record in the M1 CSV by comparing its `edit_code`, `comments`, and other relevant fields against the corresponding (simulated) rates data.
5.  Assigns a `validation_status` (e.g., "OK to Submit", "Needs Review: [Reason]", etc.) to each M1 record.
6.  Outputs a new CSV file (`M1_Shepparton_validated.csv`) containing all original M1 data plus the new `validation_status` column.

**This script is a starting point and requires customization to connect to your actual rates database for production use.**

## Prerequisites

*   Python 3.x
*   pip (Python package installer)

## Setup

1.  **Clone/Download the Script:**
    Ensure you have the `m1_validator.py` script.

2.  **Install Dependencies:**
    The script requires the `pandas` and `requests` libraries. Install them using pip:
    ```bash
    pip install pandas requests
    ```
    If you intend to connect to a Microsoft SQL Server database (as per the guidance for production use), you will also need `pyodbc`:
    ```bash
    pip install pyodbc
    ```

## Running the Script (Proof of Concept)

To run the script in its current PoC state (using sample data):
```bash
python m1_validator.py
```
This will:
*   Download the sample M1 CSV specified by `csv_url` in the script.
*   Process it against the built-in `sample_rates_data`.
*   Generate an output file named `M1_Shepparton_validated.csv` in the same directory.

## Output

The output CSV file (`M1_Shepparton_validated.csv`) will contain all the columns from the input M1 CSV, plus an additional final column:
*   `validation_status`: This column provides a text description of the validation outcome for each row based on the script's logic and the (simulated) rates data.

## Connecting to Your Actual Rates Database (IMPORTANT CUSTOMIZATION)

To make this script useful in your production environment, you **must** replace the simulated data lookup with a real database connection. The key areas to modify in `m1_validator.py` are:

1.  **Remove/Comment out `sample_rates_data`:**
    The global list `sample_rates_data` should be deleted or commented out.

2.  **Implement Real Database Logic in `get_rates_data(...)` function:**
    This function is responsible for fetching property details from your rates database. You will need to:
    *   **Establish a Database Connection:**
        *   Use a library like `pyodbc` to connect to your Microsoft SQL Server (e.g., server `vm59`, database `InfoProd`).
        *   Securely manage your database credentials (DSN, connection string with username/password, or Windows Authentication). **Do not hardcode credentials directly in the script for production.** Consider environment variables, a configuration file, or a secrets management system.
        *   An example conceptual connection setup using `pyodbc` is provided as comments within the `get_rates_data` function in the script and in the separate integration guidance.
    *   **Write SQL Queries:**
        *   Based on your database schema for `InfoProd.Infodbo.Property_Land_Use_Facts` and `InfoProd.Infodbo.Property` (or other relevant tables).
        *   Your queries should aim to fetch necessary fields like `propnum`, `spi`, `address_full`, `lot_number`, `plan_number`, `status`, and critically, the `Memo` field for a given property identifier.
        *   Use parameterized queries to prevent SQL injection.
    *   **Return Data Consistently:**
        *   The function should return a dictionary-like object (e.g., a dictionary created from a database row, or a pandas Series from a `pd.read_sql` query) if a record is found. The keys of this dictionary should match what `validate_m1_row` expects (e.g., 'propnum', 'spi', 'Memo', 'status', 'address_full').
        *   Return `None` if no matching record is found in the rates database.
    *   **Refer to the detailed guidance provided previously for example code snippets and further advice on `pyodbc` usage.**

3.  **Adjust `main()` function (if needed):**
    *   If your database connection is managed globally (e.g., opened at the start of `main` and closed at the end), ensure this is handled correctly.
    *   The call to `get_rates_data` will no longer pass `sample_rates_data`.

**Example structure for the modified `get_rates_data`:**
```python
# import pyodbc # Make sure to import

# def get_db_connection():
#     # ... (your connection logic using pyodbc.connect()) ...
#     # return cnxn_object
#     pass

def get_rates_data(propnum_csv, spi_csv, pfi_csv): # No sample_rates_data argument
    # sql_conn = get_db_connection() # Or manage connection globally
    # if not sql_conn:
    #     return None
    # cursor = sql_conn.cursor()

    # propnum_csv_clean = ...
    # spi_csv_clean = ...
    
    # YOUR SQL QUERY LOGIC HERE
    # Example:
    # if propnum_csv_clean:
    #     query = "SELECT PropNumColDb AS propnum, MemoColDb AS Memo, ... FROM YourTable WHERE PropNumColDb = ? AND StatusCol = 'C'"
    #     params = (propnum_csv_clean,)
    #     # cursor.execute(query, params)
    #     # row = cursor.fetchone()
    #     # if row:
    #     #     return dict(zip([column[0] for column in cursor.description], row))
    # (Add similar logic for SPI lookup)

    return None # If no record found after trying all identifiers
```

## Further Customization

*   **Validation Rules (`validate_m1_row` function):**
    *   The current rules are based on common `edit_code` patterns and keyword matching in memos/comments.
    *   Review and expand the `edit_code` lists and keyword lists to match the specifics of your data and council processes.
    *   You may need to implement more sophisticated logic for certain `edit_code`s or scenarios.
    *   Consider comparing `vicmap_val` and `council_val` from the M1 CSV more directly with rates data for certain validation checks.
*   **Input CSV URL:**
    *   Modify the `csv_url` variable in `main()` if your M1 CSV is located elsewhere or if you want to process local files.
*   **Output Filename:**
    *   Change `output_filename` in `main()` if needed.

This README provides a starting point. Feel free to expand it as you develop the script further.
